{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purview \u2192 scikit-learn end-to-end demo (notebook)\n",
    "\n",
    "This notebook shows how to:\n",
    "1. Discover governed data in **Microsoft Purview** and load it for training.\n",
    "2. Train a **scikit-learn** model and save artifacts to Azure Storage.\n",
    "3. Push **lineage** (input \u2192 training process \u2192 model) to Purview using the Atlas API.\n",
    "\n",
    "**Notes**:\n",
    "- Purview catalogs metadata (not your data) and powers discovery, classification, labeling, and lineage. \n",
    "- Azure Data Factory/Synapse can auto-publish runtime lineage; we add **custom lineage** here for the Python training step. \n",
    "\n",
    "References: Purview governance & catalog (Data Map/Unified Catalog) [docs](https://learn.microsoft.com/en-us/purview/data-governance-overview), Atlas lineage API [tutorial](https://learn.microsoft.com/en-us/purview/data-gov-api-create-lineage-relationships), ADF lineage [how-to](https://learn.microsoft.com/en-us/azure/data-factory/tutorial-push-lineage-to-purview).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pip"
    ]
   },
   "source": [
    "%pip install --quiet --upgrade azure-identity adlfs pandas scikit-learn requests pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os, json, uuid, io, pickle, logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from adlfs import AzureBlobFileSystem\n",
    "from purview_helpers.purview_client import PurviewAtlasClient\n",
    "from purview_helpers.policy_gate import PolicyGate\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Configuration\n",
    "Fill these based on your environment. You must have already registered & scanned your data sources in Purview.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "PURVIEW_ACCOUNT = os.environ.get('PURVIEW_ACCOUNT', '<your-purview-account-name>')\n",
    "STORAGE_ACCOUNT_NAME = os.environ.get('STORAGE_ACCOUNT_NAME', '<yourstorage>')\n",
    "CONTAINER = os.environ.get('CONTAINER', 'data')\n",
    "# Paths inside the container\n",
    "TRAIN_DATA_PATH = os.environ.get('TRAIN_DATA_PATH', 'ml/train.parquet')\n",
    "MODEL_OUTPUT_PATH = os.environ.get('MODEL_OUTPUT_PATH', 'models/churn_model.pkl')\n",
    "# Entity type names used in Purview for ADLS Gen2 and Blob path assets (adjust if needed)\n",
    "INPUT_ENTITY_TYPE = 'azure_datalake_gen2_path'\n",
    "OUTPUT_ENTITY_TYPE = 'azure_blob_path'\n",
    "# Current user (optional): object id for policy evaluation hooks; can be omitted for demo\n",
    "CURRENT_USER_OBJECT_ID = os.environ.get('CURRENT_USER_OBJECT_ID')\n",
    "print('Using Purview:', PURVIEW_ACCOUNT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Initialize clients (Purview + Storage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "credential = DefaultAzureCredential()\n",
    "pv = PurviewAtlasClient(PURVIEW_ACCOUNT, credential=credential)\n",
    "policy = PolicyGate(mode='allow_all')  # change to 'deny_writes' to see enforcement\n",
    "abfs = AzureBlobFileSystem(account_name=STORAGE_ACCOUNT_NAME, credential=credential)\n",
    "storage_options = abfs.storage_options\n",
    "print('Storage options ready for account', STORAGE_ACCOUNT_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) (Optional) Discover candidate datasets via Purview search\n",
    "Search for assets in the catalog (by keyword).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "search = pv.search_basic(keywords='churn')\n",
    "hits = [(e['typeName'], e['attributes'].get('qualifiedName')) for e in search.get('entities', [])]\n",
    "hits[:5]  # peek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Enforce read policy & load the training data from ADLS Gen2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "dataset_url = f'abfss://{CONTAINER}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{TRAIN_DATA_PATH}'\n",
    "policy.enforce(CURRENT_USER_OBJECT_ID, dataset_url, 'read')\n",
    "df = pd.read_parquet(dataset_url, storage_options=storage_options)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Train a scikit-learn pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Adjust feature/label columns for your dataset\n",
    "label_col = df.columns[-1]\n",
    "X = df.drop(columns=[label_col]).select_dtypes(include=['number']).fillna(0)\n",
    "y = df[label_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if y.nunique()<=20 else None)\n",
    "pipe = Pipeline([('scaler', StandardScaler(with_mean=False)), ('clf', LogisticRegression(max_iter=500))])\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Save the model artifact to Azure Storage (ADLS/Blob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "policy.enforce(CURRENT_USER_OBJECT_ID, f'abfss://{CONTAINER}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{MODEL_OUTPUT_PATH}', 'write')\n",
    "model_bytes = io.BytesIO(pickle.dumps(pipe))\n",
    "model_url = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{MODEL_OUTPUT_PATH}\"\n",
    "# AzureBlobFileSystem.open expects a path relative to the account; strip scheme and account host\n",
    "relative_path = model_url.replace('abfss://', '')\n",
    "with abfs.open(relative_path, 'wb') as f:\n",
    "    f.write(model_bytes.getvalue())\n",
    "print('Model written to', model_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Push lineage to Purview (input \u2192 process \u2192 output)\n",
    "This uses the Atlas entity and relationship APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "process_name = f\"sklearn_training_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
    "input_qn = f\"https://{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/{CONTAINER}/{TRAIN_DATA_PATH}\"\n",
    "output_qn = f\"https://{STORAGE_ACCOUNT_NAME}.blob.core.windows.net/{CONTAINER}/{MODEL_OUTPUT_PATH}\"\n",
    "input_guid = pv.create_or_stub_dataset(INPUT_ENTITY_TYPE, input_qn, name='training_parquet')\n",
    "proc_guid = pv.create_process(name=process_name, description='scikit-learn training run')\n",
    "output_guid = pv.create_or_stub_dataset(OUTPUT_ENTITY_TYPE, output_qn, name='sklearn_model_pkl')\n",
    "pv.add_lineage(input_guid, INPUT_ENTITY_TYPE, proc_guid, output_guid, OUTPUT_ENTITY_TYPE)\n",
    "print('Lineage submitted. GUIDs:', {'input': input_guid, 'process': proc_guid, 'output': output_guid})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- In the Purview portal, locate the **training_parquet** or **sklearn_model_pkl** assets and open the **Lineage** tab.\n",
    "- Apply **sensitivity labels** to inputs/outputs in the Data Map, and pilot **Protection policies** (preview) if you need data-plane enforcement on labeled assets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}